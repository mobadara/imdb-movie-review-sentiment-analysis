{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPBucRN1Dw0pgnk9Wwv6ra/"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Sentiment Analysis with LSTM on PyTorch**\n",
        "\n",
        "[![Python](https://img.shields.io/badge/Python-3.9%2B-blue?style=flat&logo=python&logoColor=white)](https://www.python.org/)\n",
        "[![PyTorch](https://img.shields.io/badge/PyTorch-2.0%2B-red?style=flat&logo=pytorch&logoColor=white)](https://pytorch.org/)\n",
        "[![RNN](https://img.shields.io/badge/Model-RNN%20(LSTM)-green?style=flat)](https://en.wikipedia.org/wiki/Recurrent_neural_network)\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/mobadara/imdb-movie-review-sentiment-analysis/blob/main/notebooks/model-training.ipynb)\n",
        "\n",
        "---\n",
        "\n",
        "## **End-to-End Sentiment Analysis with PyTorch LSTM**\n",
        "\n",
        "This project demonstrates an end-to-end sentiment analysis solution built using PyTorch, focusing on a binary classification problem (positive vs. negative sentiment). The primary objective of this submission is to showcase proficiency in key deep learning concepts and practical PyTorch implementation skills, including:\n",
        "\n",
        "* **Data Handling with Pandas and PyTorch:** Efficiently loading, processing, and transforming data from a Pandas DataFrame into PyTorch `TensorDataset` and `DataLoader` for effective batch processing.\n",
        "* **Comprehensive Text Preprocessing:** Implementing robust text cleaning techniques, including HTML tag removal, lowercasing, tokenization, stop word removal, and lemmatization, crucial for preparing raw text for neural network input.\n",
        "* **Vocabulary Management and Embedding Preparation:** Building a custom vocabulary and converting textual data into numerical sequences suitable for embedding layers. This includes handling padding and unknown tokens.\n",
        "* **Recurrent Neural Network (RNN) Architecture Design:** Constructing and training a Long Short-Term Memory (LSTM) network, a powerful variant of RNNs, specifically tailored for sequential data like natural language. The model will leverage an embedding layer for dense word representations.\n",
        "* **PyTorch Model Training and Evaluation:** Implementing a complete training loop, defining appropriate loss functions (Binary Cross-Entropy), optimizers (Adam), and evaluating model performance (accuracy) on a held-out test set.\n",
        "\n",
        "The [dataset](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) for this task consists of reviews with a perfectly balanced class distribution, providing an ideal scenario for training and validating a sentiment classifier. This notebook serves as a comprehensive demonstration of the entire machine learning pipeline, from raw data to a trained predictive model, using the PyTorch framework.\n",
        "\n",
        "---\n",
        "\n",
        "## **Theoretical Background: Recurrent Neural Networks (RNNs)**\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a class of artificial neural networks designed to recognize patterns in sequences of data, such as text, speech, or time series. Unlike traditional feedforward neural networks, RNNs have connections that allow information to flow in a loop, enabling them to maintain an internal state (or \"memory\") that captures information about previous elements in the sequence. This \"memory\" makes them particularly well-suited for tasks involving sequential data where the context of past elements is crucial for understanding the current one.\n",
        "\n",
        "### **Introduction: The Challenge of Vanishing/Exploding Gradients**\n",
        "\n",
        "A significant challenge with vanilla RNNs is the problem of **vanishing or exploding gradients**. During backpropagation through time (BPTT), which is how RNNs learn, gradients can either become extremely small (vanishing) or extremely large (exploding) as they propagate through many time steps.\n",
        "\n",
        "* **Vanishing gradients** make it difficult for the network to learn long-range dependencies, as the influence of earlier inputs on the current output diminishes over time.\n",
        "* **Exploding gradients** lead to unstable training and large weight updates, potentially causing the model to diverge.\n",
        "\n",
        "### **Long Short-Term Memory (LSTM) Networks**\n",
        "\n",
        "To address the vanishing gradient problem, **Long Short-Term Memory (LSTM) networks** were introduced. LSTMs are a special type of RNN that are capable of learning long-term dependencies. They achieve this through a sophisticated internal structure called a \"cell state\" and several \"gates\" that regulate the flow of information into and out of the cell state.\n",
        "\n",
        "Each LSTM unit consists of:\n",
        "* **Forget Gate:** Decides what information to throw away from the cell state.\n",
        "* **Input Gate:** Decides what new information to store in the cell state.\n",
        "* **Output Gate:** Decides what part of the cell state to output.\n",
        "\n",
        "These gates are typically composed of a sigmoid neural net layer and a pointwise multiplication operation. The sigmoid layer outputs numbers between 0 and 1, describing how much of each component should be let through. A value of 0 means \"don't let anything through,\" while a value of 1 means \"let everything through.\"\n",
        "\n",
        "By intelligently controlling the flow of information, LSTMs can preserve relevant information over long sequences, making them highly effective for tasks like sentiment analysis, machine translation, and speech recognition."
      ],
      "metadata": {
        "id": "tZmqwJxaLIpM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Setup and Library Initialization**\n",
        "This section is dedicated to the essential setup procedures required for the project. Below, we'll load all necessary Python packages, ensuring the environment is correctly configured with the appropriate settings and dependencies. This foundational step is critical for seamless execution of the subsequent data processing, model training, and evaluation phases of our sentiment analysis task."
      ],
      "metadata": {
        "id": "Dm5ZVdccRGpJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "dQyZN03hJ9Il",
        "outputId": "26ec275b-aca8-4c2b-9509-c9ffb0efb659",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Path to dataset files: /kaggle/input/imdb-dataset-of-50k-movie-reviews\n",
            "The path /kaggle/input/imdb-dataset-of-50k-movie-reviews exists: True\n",
            "Files in the path /kaggle/input/imdb-dataset-of-50k-movie-reviews: ['IMDB Dataset.csv']\n",
            "--------------------------------------------------\n",
            "PyTorch version: 2.6.0+cu124\n",
            "NLTK version: 3.9.1\n",
            "Pandas version: 2.2.2\n",
            "Matplotlib version: 3.10.0\n",
            "Setup Completed . . . \n"
          ]
        }
      ],
      "source": [
        "# import packages\n",
        "import os\n",
        "import torch\n",
        "import nltk\n",
        "import re\n",
        "import kagglehub\n",
        "import matplotlib\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "plt.style.use('ggplot')\n",
        "torch.manual_seed(42)\n",
        "\n",
        "# Download latest version Dataset\n",
        "path = kagglehub.dataset_download(\"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\")\n",
        "print(\"Path to dataset files:\", path)\n",
        "print(f'The path {path} exists: {os.path.exists(path)}')\n",
        "print(f'Files in the path {path}: {os.listdir(path)}')\n",
        "\n",
        "print(f'-'*50)\n",
        "print(f'PyTorch version: {torch.__version__}')\n",
        "print(f'NLTK version: {nltk.__version__}')\n",
        "print(f'Pandas version: {pd.__version__}')\n",
        "print(f'Matplotlib version: {matplotlib.__version__}')\n",
        "print('Setup Completed . . . ')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "owMQF-zyi88e",
        "outputId": "d9d21427-91af-4df7-ca56-c21cd6965b8b"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cpu')"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Loading**\n",
        "\n",
        "In this crucial step, we will load our dataset, which has been pre-downloaded and stored within the specified directory. The file path to this dataset is dynamically managed and accessible via the `path` variable, ensuring flexibility and ease of access. We will utilize the `pandas` library to efficiently read the data into a DataFrame, providing a robust structure for subsequent preprocessing and analysis. This initial data ingestion is fundamental to preparing our text reviews for the sentiment analysis pipeline."
      ],
      "metadata": {
        "id": "cpnh0IJ7WVWy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Load the data from the specified path into a Pandas DataFrame\n",
        "    full_path = os.path.join(path, 'IMDB Dataset.csv')\n",
        "    df = pd.read_csv(full_path)\n",
        "    print(f\"Successfully loaded data from: {full_path}\")\n",
        "    print(\"DataFrame Head:\")\n",
        "    print(df.head())\n",
        "    print(\"\\nDataFrame Info:\")\n",
        "    df.info()\n",
        "except FileNotFoundError:\n",
        "    print(f\"Error: The file '{full_path}' was not found.\")\n",
        "    print(\"Please ensure the file exists at the specified path.\")\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while loading the data: {e}\")"
      ],
      "metadata": {
        "id": "sxrzBvuyUEWC",
        "outputId": "1f4bf588-9f7e-4032-9421-58cf9f91a568",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Successfully loaded data from: /kaggle/input/imdb-dataset-of-50k-movie-reviews/IMDB Dataset.csv\n",
            "DataFrame Head:\n",
            "                                              review sentiment\n",
            "0  One of the other reviewers has mentioned that ...  positive\n",
            "1  A wonderful little production. <br /><br />The...  positive\n",
            "2  I thought this was a wonderful way to spend ti...  positive\n",
            "3  Basically there's a family where a little boy ...  negative\n",
            "4  Petter Mattei's \"Love in the Time of Money\" is...  positive\n",
            "\n",
            "DataFrame Info:\n",
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 50000 entries, 0 to 49999\n",
            "Data columns (total 2 columns):\n",
            " #   Column     Non-Null Count  Dtype \n",
            "---  ------     --------------  ----- \n",
            " 0   review     50000 non-null  object\n",
            " 1   sentiment  50000 non-null  object\n",
            "dtypes: object(2)\n",
            "memory usage: 781.4+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This dataset comprises **50,000** reviews, each accompanied by a sentiment label indicating whether the review is positive or negative."
      ],
      "metadata": {
        "id": "pKV5e_XSYcur"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ax = df.sentiment.value_counts().plot(kind='bar', color='blue', title='Count of Reviews by Sentiment')\n",
        "ax.set_xlabel('Sentiment')\n",
        "ax.set_ylabel('Number of Reviews')\n",
        "ax.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "m66jfCxBYWkJ",
        "outputId": "c083df99-4c60-4819-e1ad-903cfeb787fd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 524
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Observation on Class Balance**\n",
        "\n",
        "As visually confirmed by the bar chart above, the dataset exhibits a perfectly balanced class distribution between positive and negative sentiments. This crucial characteristic implies that both sentiment classes are equally represented, mitigating common challenges associated with imbalanced datasets such as model bias towards the majority class. This balanced nature will facilitate straightforward training without the immediate need for advanced sampling techniques (e.g., oversampling, undersampling) or specialized loss functions, contributing to a more robust and fair evaluation of our sentiment analysis model."
      ],
      "metadata": {
        "id": "Na2-OmL2aqoG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "reviews = df['review'].tolist()\n",
        "sentiments = df['sentiment'].tolist()"
      ],
      "metadata": {
        "id": "4FxVg9k_ZcNB"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**\n",
        "\n",
        "The `preprocess_text` function, shown below, is designed to clean up the raw movie reviews and get them ready for the model. This is a crucial step because it takes messy text and makes it consistent, while keeping important parts like emoticons.\n",
        "\n",
        "Let's break down what each part of this function does:\n",
        "\n",
        "- **Remove HTML Tags**: First, it gets rid of any HTML tags (like `<br/>` or `<i>`). These tags are just formatting and don't tell us anything about the sentiment.\n",
        "\n",
        "- **Lowercase Everything**: All text is converted to lowercase. This means words like \"Good,\" \"good,\" and \"GOOD\" are treated the same, which helps keep the vocabulary smaller and more consistent.\n",
        "\n",
        "- **Grab and Keep Emoticons**: This is a special part of the function. It finds emoticons (like `:)` or `:D`) and saves them. Emoticons are super important for sentiment, so we make sure they stick around even after we clean up other punctuation.\n",
        "\n",
        "- **Clean Up Other Characters**: After saving the emoticons, the function removes most other non-letter characters (like numbers or extra spaces) and replaces them with a single space. This further tidies up the text.\n",
        "\n",
        "- **Break into Words (Tokenize)**: The cleaned text is then split into individual words or \"tokens.\" This turns a big block of text into a list of words, which is what our model needs.\n",
        "\n",
        "- **Remove Stop Words**: Common words that don't add much meaning to the sentiment (like \"the,\" \"is,\" \"and\") are taken out. This helps the model focus on the more important words.\n",
        "\n",
        "- **Get Root Words (Lemmatize)**: Finally, words are changed to their basic form. For example, \"running,\" \"runs,\" and \"ran\" all become \"run.\" This helps reduce the total number of unique words while keeping their core meaning."
      ],
      "metadata": {
        "id": "eh0UdWAr4RMV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(text: str) -> None:\n",
        "    # Remove HTML tags\n",
        "    text = re.sub(r'<.*?>', '', text)\n",
        "    # Convert to lowercase\n",
        "    text = text.lower()\n",
        "    # Extract emoticons characters\n",
        "    emoticons = re.findall('(?::|;|=)(?:-)?(?:\\)|\\(|D|P)', text)\n",
        "    text = re.sub('[\\W]+', ' ', text.lower()) + ' '.join(emoticons).replace('-', '')\n",
        "    # Tokenize the text\n",
        "    tokens = word_tokenize(text)\n",
        "    # Remove Stop Words\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    tokens = [word for word in tokens if word not in stop_words]\n",
        "    # Convert words to their root form (stemmilng)\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "    return tokens"
      ],
      "metadata": {
        "id": "Ojm-9RKJclih"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that our `preprocess_text` function is defined, it's time to apply it to all the movie reviews we loaded. This step transforms our raw text into a clean, tokenized format that our model can understand. Right after that, we'll convert our \"positive\" and \"negative\" sentiment labels into numbers, which is essential for PyTorch."
      ],
      "metadata": {
        "id": "RIW2qUTf-DHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply preprocessing to your reviews\n",
        "preprocessed_reviews = [preprocess_text(review) for review in reviews]\n",
        "\n",
        "# Convert sentiments to numerical labels\n",
        "sentiment_labels = [1 if sentiment == 'positive' else 0 for sentiment in sentiments]\n"
      ],
      "metadata": {
        "id": "ZLQqc2CT7qL3"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Splitting Data into Training and Testing Sets**\n",
        "\n",
        "With our reviews preprocessed and sentiments converted to numerical labels, the next crucial step is to divide our dataset into distinct training and testing sets. This separation is fundamental in machine learning to ensure that we can evaluate our model's performance on unseen data, providing an honest assessment of its generalization capabilities.\n",
        "\n",
        "We'll use `sklearn.model_selection.train_test_split` for this task."
      ],
      "metadata": {
        "id": "lBswuql9-Ykl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_reviews, test_reviews, train_labels, test_labels = train_test_split(preprocessed_reviews,\n",
        "                                                                sentiment_labels, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "CWpHIjZEBZgl"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Building the Vocabulary and Word-to-Index Mapping**\n",
        "\n",
        "Now that our data is split, the next step is to create a vocabulary from our training reviews. This vocabulary will essentially be a unique list of every word our model has seen during training. Once we have this list, we'll assign a unique numerical ID (an index) to each word. This numerical representation is crucial because neural networks, including LSTMs, can only process numbers, not raw text.\n",
        "\n",
        "We'll also add special tokens for padding and unknown words, which are essential for consistent input to our model."
      ],
      "metadata": {
        "id": "p6_hcebGA8xN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = set(word for review in train_reviews for word in review)\n",
        "word_to_index = {word: index + 2 for index, word in enumerate(vocab)}\n",
        "word_to_index['<PAD>'] = 0\n",
        "word_to_index['<UNK>'] = 1"
      ],
      "metadata": {
        "id": "zRepUlb9-met"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Converting Reviews to Numerical Sequences**\n",
        "\n",
        "Now that we have our word_to_index mapping, we can finally convert our text reviews into numerical sequences. This is the stage where words transform into numbers, a format that our PyTorch **LSTM model** can actually process. We'll do this for both our training and testing reviews."
      ],
      "metadata": {
        "id": "EjwLjjqmB-8I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = [[word_to_index.get(word, word_to_index['<UNK>']) for word in review] for review in train_reviews]\n",
        "test_sequences = [[word_to_index.get(word, word_to_index['<UNK>']) for word in review] for review in test_reviews]\n"
      ],
      "metadata": {
        "id": "2Gz90GRUBy1t"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Padding Sequences to Uniform Length**\n",
        "\n",
        "Deep learning models, especially those operating on batches of data like LSTMs, require all input sequences within a batch to have the same fixed length. Since our movie reviews vary in length, we need to apply padding. This process involves extending shorter sequences with a special \"padding\" token until they match the length of the longest sequence in our training set."
      ],
      "metadata": {
        "id": "aZHDi0pHCXQF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = max(len(seq) for seq in train_sequences)\n",
        "train_sequences = [seq + [word_to_index['<PAD>']] * (max_length - len(seq)) for seq in train_sequences]\n",
        "test_sequences = [seq + [word_to_index['<PAD>']] * (max_length - len(seq)) for seq in test_sequences]\n"
      ],
      "metadata": {
        "id": "SV9uM_3XAP0R"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Converting Data to PyTorch Tensors**\n",
        "The final step in preparing our data is to convert our numerical Python lists into PyTorch tensors. Tensors are the fundamental data structures in PyTorch, similar to NumPy arrays, but with the added capability of being used on GPUs for accelerated computation, which is essential for training deep learning models."
      ],
      "metadata": {
        "id": "sQeZ1xgYCyOj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences = torch.tensor(train_sequences)\n",
        "train_labels = torch.tensor(train_labels)\n",
        "test_sequences = torch.tensor(test_sequences)\n",
        "test_labels = torch.tensor(test_labels)\n"
      ],
      "metadata": {
        "id": "oY3gXOkMCreH"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Defining the LSTM Model Architecture**\n",
        "\n",
        "This section defines the neural network architecture for our sentiment analysis task. We're using a Long Short-Term Memory (LSTM) network, which is particularly well-suited for processing sequential data like text. The model is encapsulated within the SentimentAnalysisLSTM class, inheriting from PyTorch's `nn.Module`."
      ],
      "metadata": {
        "id": "d9vFwDMXXxNT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the LSTM Model\n",
        "class SentimentAnalysisLSTM(nn.Module):\n",
        "    def __init__(self, vocab_size: int, embedding_dim: int, hidden_dim: int, output_dim: int, n_layers: int, bidirectional: bool, dropout: float) -> None:\n",
        "        \"\"\"\n",
        "        Initializes the SentimentAnalysisLSTM model.\n",
        "\n",
        "        Args:\n",
        "            vocab_size (int): The number of unique words in the vocabulary.\n",
        "            embedding_dim (int): The size of the word embeddings.\n",
        "            hidden_dim (int): The number of features in the hidden state of the LSTM.\n",
        "            output_dim (int): The size of the output layer (1 for binary classification).\n",
        "            n_layers (int): The number of LSTM layers.\n",
        "            bidirectional (bool): If True, the LSTM will be bidirectional.\n",
        "            dropout (float): The dropout probability for regularization.\n",
        "        \"\"\"\n",
        "        super(SentimentAnalysisLSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, bidirectional=bidirectional, dropout=dropout, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, text: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Forward pass of the model.\n",
        "\n",
        "        Args:\n",
        "            text (torch.Tensor): The input text data.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: The output of the model.\n",
        "        \"\"\"\n",
        "        embedded = self.embedding(text)\n",
        "        output, (hidden, cell) = self.lstm(embedded)\n",
        "        hidden = torch.cat((hidden[-2, :, :], hidden[-1, :, :]), dim=1) if self.lstm.bidirectional else hidden[-1, :, :]\n",
        "        return self.sigmoid(self.fc(hidden))"
      ],
      "metadata": {
        "id": "phuGsYBRDLSs"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Explanation of Components**\n",
        "\n",
        "1. **`nn.Embedding Layer`**:\n",
        "This is the first layer, responsible for taking our numerical word indices and converting them into dense, continuous vectors (embeddings). Each word in our vocabulary will have its own learned vector representation, capturing semantic relationships between words.\n",
        "\n",
        "2. **`nn.LSTM Layer`**:\n",
        "This is the core of our recurrent network. It takes the word embeddings as input.\n",
        "\n",
        "- **`hidden_dim`**: Determines the dimensionality of the `hidden state`, which acts as the \"memory\" of the LSTM.\n",
        "\n",
        "- **`num_layers`**: Allows stacking multiple LSTM layers on top of each other, enabling the model to learn more complex patterns.\n",
        "\n",
        "- **`bidirectional`**: A crucial parameter that enables the LSTM to process the input sequence in both forward and backward directions. This means the model can capture context from words appearing both before and after the current word in a sentence, which is highly beneficial for understanding sentiment.\n",
        "\n",
        "- **`dropout`**: A regularization technique applied to the output of each LSTM layer to prevent overfitting by randomly setting a fraction of outputs to zero during training.\n",
        "\n",
        "- **`batch_first=True`**: This setting is convenient because it means our input tensors will be shaped (`batch_size`, `sequence_length`, `embedding_dim`), which aligns well with common data loading patterns.\n",
        "\n",
        "3. **`nn.Linear`** (Fully Connected) Layer:\n",
        "After the LSTM processes the entire sequence, we are interested in its final representation. For sentiment classification, we typically take the final hidden state(s) of the LSTM. This linear layer takes that final hidden state and transforms it into our desired output_dim (which is 1 for binary classification). If the LSTM is bidirectional, the hidden state from both directions are concatenated, doubling its size, which is accounted for in the hidden_dim * 2 calculation.\n",
        "\n",
        "4. **`nn.Sigmoid`** Activation Function:\n",
        "Since this is a binary classification problem (positive or negative), we use a sigmoid function on the output of the final linear layer. The sigmoid function squashes the output value into a range between 0 and 1, which can then be interpreted as the probability of the review being positive.\n",
        "\n",
        "Forward Pass (`forward` method):\n",
        "The forward method defines the flow of data through the network:\n",
        "\n",
        "1. Embeddings: The input text (a batch of numerical sequences) first goes through the embedding layer, turning word indices into dense vectors.\n",
        "\n",
        "2. LSTM Processing: These embeddings are then fed into the lstm layer. The LSTM processes the sequence, maintaining an internal \"memory\" and producing an output (the output of all time steps) and the final hidden and cell states.\n",
        "\n",
        "3. Extract Final Hidden State: For sequence classification tasks like sentiment analysis, we typically use the final hidden state of the LSTM as the aggregated representation of the entire sequence.\n",
        "\n",
        "- If the LSTM is bidirectional, the `hidden` tensor contains states for both forward and backward directions across all layers. We specifically take the final hidden states from the last forward layer (`hidden[-2, :, :]`) and the last backward layer (`hidden[-1, :, :]`) and concatenate them to form a single, richer representation.\n",
        "\n",
        "- If unidirectional, we simply take the hidden state from the last layer (`hidden[-1, :, :]`).\n",
        "\n",
        "- Final Prediction: This combined (or single) final hidden state is then passed through the `fc` (fully connected) layer, and a `sigmoid` activation is applied to produce the final sentiment probability for each review in the batch."
      ],
      "metadata": {
        "id": "4N4xJHhqYKv4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training the Sentiment Analysis LSTM Model**\n",
        "\n",
        "This section details the training and validation process for our SentimentAnalysisLSTM model. It includes the setup for splitting data into train and validation sets, defining hyperparameters, initializing the model components, executing the training loop, and comprehensive evaluation with key metrics and visualizations."
      ],
      "metadata": {
        "id": "IjqQsZUOhz6_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Hyperparameter Configuration and Device Setup**\n",
        "Core hyperparameters such as `vocab_size`, `embedding_dim`, `hidden_dim`, `n_layers`, `bidirectional`, `dropout`, `learning_rate`, `batch_size`, and `epochs` are defined. These parameters are crucial for controlling the model's architecture and the learning process. The code automatically leverages a CUDA-enabled GPU if available, falling back to CPU otherwise, to ensure efficient computation."
      ],
      "metadata": {
        "id": "9m7Z3rTtOvOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameters\n",
        "vocab_size = len(word_to_index)\n",
        "embedding_dim = 100\n",
        "hidden_dim = 256\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "bidirectional = True\n",
        "dropout = 0.5\n",
        "learning_rate = 0.001\n",
        "batch_size = 32\n",
        "num_epochs = 8"
      ],
      "metadata": {
        "id": "OlTWFvrkUUZd",
        "outputId": "2654811c-6a02-42c4-fe11-7cc0ace4159e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'word_to_index' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-2394323508>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Hyperparameters\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mvocab_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_to_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0membedding_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mhidden_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m256\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moutput_dim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'word_to_index' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Preparation for Training, Validation, and Testing**\n",
        "\n",
        "The `train_sequences` and `train_labels` are further partitioned into distinct training (80%) and validation (20%) sets. This separation is vital for monitoring model performance on unseen data during training, helping to detect overfitting. `TensorDataset` and `DataLoader` are used to efficiently manage and batch the data for all three sets (training, validation, and the final test set)."
      ],
      "metadata": {
        "id": "KjK7SpxhPKnw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_sequences_split, val_sequences, train_labels_split, val_labels = train_test_split(\n",
        "    train_sequences, train_labels, test_size=0.2, random_state=42\n",
        ")"
      ],
      "metadata": {
        "id": "3fjcVFhPi2Ma"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = TensorDataset(train_sequences_split, train_labels_split)\n",
        "val_dataset = TensorDataset(val_sequences, val_labels)\n",
        "test_dataset = TensorDataset(test_sequences, test_labels)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "metadata": {
        "id": "C2ywV_Y_lBXO"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = SentimentAnalysisLSTM(vocab_size, embedding_dim, hidden_dim, output_dim, n_layers, bidirectional, dropout).to(device)\n",
        "criterion = nn.BCELoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "TRkeYDNclHWn"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Model, Loss Function, and Optimizer Initialization**\n",
        "An instance of the `SentimentAnalysisLSTM` model is created with the specified hyperparameters and moved to the chosen device. `nn.BCELoss()` (Binary Cross-Entropy Loss) is selected as the criterion for measuring the difference between predicted probabilities and true binary labels, suitable for binary classification. The optim.Adam optimizer is employed to update model weights based on calculated gradients, with a defined learning_rate."
      ],
      "metadata": {
        "id": "H0eDdt3ePqHS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accuracies = []\n",
        "val_accuracies = []\n",
        "val_precisions = []\n",
        "val_recalls = []\n",
        "\n",
        "MODEL_SAVE_DIR = 'models'\n",
        "os.makedirs(MODEL_SAVE_DIR, exist_ok=True)"
      ],
      "metadata": {
        "id": "qbQF95lKlk_o"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training and Validation Loop**\n",
        "\n",
        "The training process iterates over a fixed number of `epochs`. Each epoch comprises two phases:\n",
        "\n",
        "1. **Training Phase (`model.train()`)**:\n",
        "\n",
        "- The model is set to training mode, enabling dropout.\n",
        "- For each mini-batch from the `train_loader`:\n",
        " - Input sequences and labels are moved to the device.\n",
        " - Previous gradients are cleared (`optimizer.zero_grad()`).\n",
        " - Predictions are generated.\n",
        " - The loss is calculated, and backpropagation (`loss.backward()`) computes gradients.\n",
        " - Optimizer updates model parameters (`optimizer.step()`).\n",
        "- Training loss and predicted labels are accumulated to calculate epoch-level training accuracy.\n",
        "\n",
        "2. **Validation Phase (`model.eval()`)**:\n",
        "\n",
        "- The model is switched to evaluation mode, disabling training-specific layers like dropout.\n",
        "- `torch.no_grad()` is activated to prevent gradient computations, optimizing performance for inference.\n",
        "- For each mini-batch from the val_loader:\n",
        " - Predictions are made, and loss is computed.\n",
        " - Validation loss, predicted labels, and true labels are accumulated.\n",
        "- At the end of the validation phase, Validation Accuracy, Precision, and Recall are calculated using sklearn.metrics. These metrics provide a comprehensive view of the model's performance on the validation set, especially regarding its ability to correctly identify positive instances (Precision) and its coverage of all actual positive instances (Recall).\n",
        "- Progress is printed for each epoch, showing training and validation loss, accuracy, precision, and recall.\n",
        "- The state of the model at the end of each epoch is saved for reference."
      ],
      "metadata": {
        "id": "tHrF9XFDQSSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print('\\nStarting training...')\n",
        "for epoch in tqdm(range(num_epochs), desc=f'Training . . . '):\n",
        "    # Training phase\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    all_train_predictions = []\n",
        "    all_train_labels = []\n",
        "\n",
        "    for sequences, labels in train_loader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(sequences).squeeze(1)\n",
        "        loss = criterion(predictions, labels.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * labels.size(0)\n",
        "        all_train_predictions.extend(torch.round(predictions).detach().cpu().numpy())\n",
        "        all_train_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_train_loss = running_loss / len(train_loader.dataset)\n",
        "    epoch_train_accuracy = accuracy_score(all_train_labels, all_train_predictions)\n",
        "    train_losses.append(epoch_train_loss)\n",
        "    train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "\n",
        "    # Validation phase\n",
        "    model.eval()\n",
        "    val_running_loss = 0.0\n",
        "    all_val_predictions = []\n",
        "    all_val_labels = []\n",
        "    with torch.no_grad():\n",
        "        for sequences, labels in val_loader:\n",
        "            sequences, labels = sequences.to(device), labels.to(device)\n",
        "            predictions = model(sequences).squeeze(1)\n",
        "            loss = criterion(predictions, labels.float())\n",
        "\n",
        "            val_running_loss += loss.item() * labels.size(0)\n",
        "            all_val_predictions.extend(torch.round(predictions).cpu().numpy())\n",
        "            all_val_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    epoch_val_loss = val_running_loss / len(val_loader.dataset)\n",
        "    epoch_val_accuracy = accuracy_score(all_val_labels, all_val_predictions)\n",
        "    epoch_val_precision = precision_score(all_val_labels, all_val_predictions, zero_division=0)\n",
        "    epoch_val_recall = recall_score(all_val_labels, all_val_predictions, zero_division=0)\n",
        "\n",
        "    val_losses.append(epoch_val_loss)\n",
        "    val_accuracies.append(epoch_val_accuracy)\n",
        "    val_precisions.append(epoch_val_precision)\n",
        "    val_recalls.append(epoch_val_recall)\n",
        "\n",
        "    print(f'Epoch {epoch+1}/{num_epochs} | '\n",
        "          f'Train Loss: {epoch_train_loss:.4f} | Train Acc: {epoch_train_accuracy:.4f} | '\n",
        "          f'Val Loss: {epoch_val_loss:.4f} | Val Acc: {epoch_val_accuracy:.4f} | '\n",
        "          f'Val Prec: {epoch_val_precision:.4f} | Val Rec: {epoch_val_recall:.4f}')\n",
        "\n",
        "    # Save Model After Each Epoch\n",
        "    model_path = os.path.join(MODEL_SAVE_DIR, f'sentiment-lstm-epoch-{epoch+1}.pt')\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "\n",
        "print('\\nTraining complete.')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T4yuHGJ1lubi",
        "outputId": "1e550f4c-3d95-42e4-c00f-308dd9f52610"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rTraining . . . :   0%|          | 0/8 [00:00<?, ?it/s]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plotting Training History**\n",
        "\n",
        "Now that  the training loop have concluded, `matplotlib` is used to visualize the model's learning trajectory. Three plots are generated:\n",
        "\n",
        "- **Loss Curve**: Illustrates the training and validation loss over epochs.'\n",
        "\n",
        "- **Accuracy Curve**: Shows the training and validation accuracy over epochs.\n",
        "\n",
        "- **Validation Precision and Recall Curve**: Tracks the validation precision and recall values over epochs, allowing for observation of their trends throughout training.\n",
        "\n",
        "These plots are critical for diagnosing issues such as overfitting (where validation loss increases or accuracy/precision/recall plateaus/decreases while training metrics continue to improve).\n"
      ],
      "metadata": {
        "id": "UKCVB9_XSSG_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Plotting Results ---\n",
        "epochs_range = range(1, epochs + 1)\n",
        "\n",
        "plt.figure(figsize=(21, 6)) # Increased figure size for 3 plots\n",
        "\n",
        "# Plotting Loss\n",
        "plt.subplot(1, 3, 1) # Changed to 1 row, 3 columns\n",
        "plt.plot(epochs_range, train_losses, label='Training Loss')\n",
        "plt.plot(epochs_range, val_losses, label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plotting Accuracy\n",
        "plt.subplot(1, 3, 2) # Changed to 1 row, 3 columns\n",
        "plt.plot(epochs_range, train_accuracies, label='Training Accuracy')\n",
        "plt.plot(epochs_range, val_accuracies, label='Validation Accuracy')\n",
        "plt.title('Training and Validation Accuracy')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "# Plotting Precision and Recall\n",
        "plt.subplot(1, 3, 3) # New subplot\n",
        "plt.plot(epochs_range, val_precisions, label='Validation Precision', color='purple')\n",
        "plt.plot(epochs_range, val_recalls, label='Validation Recall', color='orange')\n",
        "plt.title('Validation Precision and Recall')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Score')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rbc5DI0ZmLyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Final Evaluation on Test Set**\n",
        "To provide an unbiased assessment of the model's generalization capability, a final evaluation is performed on the completely unseen test_dataset. The model remains in `eval()` mode, and predictions are gathered. The Final Test Accuracy, Precision, and Recall are calculated and printed, offering a robust measure of the model's expected performance on new, real-world data.\n",
        "\n",
        "The `zero_division=0` parameter in `precision_score` and `recall_score` is used to handle potential scenarios where there might be no true positive or predicted positive samples, preventing division-by-zero errors and returning a score of 0 in such cases.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "U8J0no-2TZhT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Final Evaluation on Test Set\n",
        "print('\\nEvaluating on test set\\n...')\n",
        "model.eval()\n",
        "all_test_predictions = []\n",
        "all_test_labels = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for sequences, labels in test_loader:\n",
        "        sequences, labels = sequences.to(device), labels.to(device)\n",
        "        predictions = model(sequences).squeeze(1)\n",
        "        predicted = torch.round(predictions)\n",
        "        all_test_predictions.extend(predicted.cpu().numpy())\n",
        "        all_test_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "test_accuracy = accuracy_score(all_test_labels, all_test_predictions)\n",
        "test_precision = precision_score(all_test_labels, all_test_predictions, zero_division=0)\n",
        "test_recall = recall_score(all_test_labels, all_test_predictions, zero_division=0)\n",
        "\n",
        "print(f'Final Test Accuracy: {test_accuracy:.4f}')\n",
        "print(f'Final Test Precision: {test_precision:.4f}')\n",
        "print(f'Final Test Recall: {test_recall:.4f}')"
      ],
      "metadata": {
        "id": "IxzOIvE3mS_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PeIi4m7u5_e5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}